{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/repos/unilm/simlm/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/traindata/maksim/repos/unilm/simlm/src\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from typing import Dict\n",
    "from functools import partial\n",
    "from transformers.utils.logging import enable_explicit_format\n",
    "from transformers.trainer_callback import PrinterCallback\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    PreTrainedTokenizerFast\n",
    ")\n",
    "\n",
    "from logger_config import logger, LoggerCallback\n",
    "from config import Arguments\n",
    "from trainers import BiencoderTrainer\n",
    "from loaders import RetrievalDataLoader\n",
    "from collators import BiencoderCollator\n",
    "from metrics import accuracy, batch_mrr\n",
    "from models import BiencoderModel\n",
    "\n",
    "def _common_setup(args: Arguments):\n",
    "    if args.process_index > 0:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    enable_explicit_format()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "\n",
    "def _compute_metrics(args: Arguments, eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    # field consistent with BiencoderOutput\n",
    "    preds = eval_pred.predictions\n",
    "    scores = torch.tensor(preds[-1]).float()\n",
    "    labels = torch.arange(0, scores.shape[0], dtype=torch.long) * args.train_n_passages\n",
    "    labels = labels % scores.shape[1]\n",
    "\n",
    "    topk_metrics = accuracy(output=scores, target=labels, topk=(1, 3))\n",
    "    mrr = batch_mrr(output=scores, target=labels)\n",
    "\n",
    "    return {'mrr': mrr, 'acc1': topk_metrics[0], 'acc3': topk_metrics[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_DIR\"] = \"./data/msmarco_bm25_official/\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"./tmp/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arguments(output_dir='/traindata/maksim/repos/unilm/simlm/tmp/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=1000, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/traindata/maksim/repos/unilm/simlm/tmp/runs/Nov11_12-05-42_ip-172-19-213-218', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=500, save_total_limit=2, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=1234, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, dataloader_prefetch_factor=None, past_index=-1, run_name='/traindata/maksim/repos/unilm/simlm/tmp/', disable_tqdm=True, remove_unused_columns=False, label_names=['labels'], load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, gradient_accumulation_kwargs=None), deepspeed='/traindata/maksim/repos/unilm/simlm/ds_config.json', label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, model_name_or_path='intfloat/simlm-base-msmarco', data_dir='/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/', task_type='ir', train_file='/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/train.jsonl', validation_file='/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/dev.jsonl', train_n_passages=16, share_encoder=True, use_first_positive=False, use_scaled_loss=True, loss_scale=-1.0, add_pooler=False, out_dimension=768, t=0.02, l2_normalize=True, t_warmup=False, full_contrastive_loss=True, do_encode=False, encode_in_path=None, encode_save_dir=None, encode_shard_size=2000000, encode_batch_size=256, do_search=False, search_split='dev', search_batch_size=128, search_topk=200, search_out_dir='', do_rerank=False, rerank_max_length=256, rerank_in_path='', rerank_out_path='', rerank_split='dev', rerank_batch_size=128, rerank_depth=1000, rerank_forward_factor=1, rerank_use_rdrop=False, do_kd_gen_score=False, kd_gen_score_split='dev', kd_gen_score_batch_size=128, kd_gen_score_n_neg=30, do_kd_biencoder=False, kd_mask_hn=True, kd_cont_loss_weight=1.0, rlm_generator_model_name='google/electra-base-generator', rlm_freeze_generator=True, rlm_generator_mlm_weight=0.2, all_use_mask_token=False, rlm_num_eval_samples=4096, rlm_max_length=144, rlm_decoder_layers=2, rlm_encoder_mask_prob=0.3, rlm_decoder_mask_prob=0.5, q_max_len=32, p_max_len=144, max_train_samples=None, dry_run=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['src/train_biencoder.py', '--deepspeed', '/traindata/maksim/repos/unilm/simlm/ds_config.json', '--model_name_or_path', 'intfloat/simlm-base-msmarco', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '32', '--add_pooler', 'False', '--t', '0.02', '--seed', '1234', '--do_train', '--fp16', '--train_file', '/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/train.jsonl', '--validation_file', '/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/dev.jsonl', '--q_max_len', '32', '--p_max_len', '144', '--train_n_passages', '16', '--dataloader_num_workers', '1', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--use_scaled_loss', 'True', '--warmup_steps', '1000', '--share_encoder', 'True', '--logging_steps', '50', '--output_dir', '/traindata/maksim/repos/unilm/simlm/tmp/', '--data_dir', '/traindata/maksim/repos/unilm/simlm/data/msmarco_bm25_official/', '--save_total_limit', '2', '--save_strategy', 'epoch', '--evaluation_strategy', 'epoch', '--remove_unused_columns', 'False', '--overwrite_output_dir', '--disable_tqdm', 'True', '--report_to', 'none']\n",
    "parser = HfArgumentParser((Arguments,))\n",
    "args: Arguments = parser.parse_args_into_dataclasses()[0]\n",
    "#args.local_rank = -1 # disable dist training, for debugging!\n",
    "_common_setup(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[WARNING|modeling_utils.py:4172] 2024-11-11 12:05:43,264 >> Some weights of BertModel were not initialized from the model checkpoint at intfloat/simlm-base-msmarco and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-11-11 12:05:43,291 INFO] BiencoderModel(\n",
      "  (lm_q): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lm_p): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cross_entropy): CrossEntropyLoss()\n",
      "  (kl_loss_fn): KLDivLoss()\n",
      "  (pooler): Identity()\n",
      ")\n",
      "[2024-11-11 12:05:43,299 INFO] Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "model: BiencoderModel = BiencoderModel.build(args=args)\n",
    "logger.info(model)\n",
    "logger.info('Vocab size: {}'.format(len(tokenizer)))\n",
    "\n",
    "data_collator = BiencoderCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if args.fp16 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11 12:05:43,542 INFO] Sample 231070 of the training set: {'query_id': '1179605', 'query': 'average cost of fence homeadvisor', 'positives': {'doc_id': ['4435119'], 'score': [-1.0]}, 'negatives': {'doc_id': ['905674', '6466825', '5013998', '6008466', '7792504', '6381090', '5323265', '6202403', '8409672', '3529117', '1786957', '6098973', '5041970', '2113635', '7383823', '2046830', '6953357', '3318649', '5570938', '5890165', '2349672', '2975852', '6831044', '5653389', '6242690', '5014719', '571773', '1729765', '3472300', '5537537', '395723', '4322414', '8247821', '5471572', '6509108', '8311952', '3860934', '757714', '1097619', '6491138', '5182081', '1493425', '7057757', '1326609', '4323128', '4479581', '1347526', '6615847', '5314570', '757716', '6439609', '5841024', '6014361', '6471506', '3756582', '1851271', '4106998', '5570467', '5493263', '4257654', '7938907', '3613226', '2104322', '7749241', '1697703', '4581371', '1889331', '2559842', '7990737', '4094925', '3809462', '6913103', '6081512', '5493262', '5703571', '3151351', '4975687', '7525547', '4860969', '8138579', '3611859', '6219887', '3026938', '3151356', '5146086', '5733960', '7551554', '542443', '7482558', '8138580', '7482559', '4830276', '8658411', '1177692', '5733957', '5733429', '1708775', '7183979', '767710', '3667310', '6509304', '5841028', '841626', '6119330', '5361834', '229383', '7545053', '1493420', '1695046', '7374869', '2399530', '7692687', '281225', '5155391', '2238154', '5812259', '1982029', '6276677', '5814102', '571770', '4479580', '6469034', '1335665', '3177679', '3329484', '4627234', '4553925', '5979703', '3410088', '1795428', '1851269', '1493423', '4594440', '6327541', '6485197', '5841026', '5323263', '7384496', '6492280', '7646959', '5361831', '299585', '760748', '4411355', '6849226', '3274227', '950846', '2407788', '5520380', '454868', '5796193', '2480994', '664323', '6012768', '4435115', '3473515', '395724', '6609782', '3274228', '6417338', '1029021', '709065', '3028995', '5841027', '7536462', '6485202', '2490438', '5110713', '5743645', '3485254', '439297', '196116', '1058522', '6002470', '8276564', '5527661', '4435116', '4166082', '5608559', '8132239', '6615179', '3897056', '5743646', '6515923', '7749240', '2957120', '1177691', '5797935', '6219888', '6908929', '4594444', '6277908', '6242689', '6485204', '1732071', '6509303', '6119333', '2475102', '5796200', '439298'], 'score': [0.0062, 0.1667, 0.0081, 0.0061, 0.0526, 0.0196, 1.0, 0.0076, 0.0093, 0.0057, 0.0067, 0.0625, 0.0072, 0.0082, 0.0063, 0.0172, 0.0072, 0.25, 0.0103, 0.0161, 0.0323, 0.0263, 0.0056, 0.0099, 0.0182, 0.0068, 0.0106, 0.0094, 0.0278, 0.0455, 0.0588, 0.0053, 0.0078, 0.5, 0.0067, 0.0079, 0.0175, 0.0075, 0.0213, 0.0061, 0.0147, 0.0087, 0.0075, 0.0092, 0.0141, 0.0066, 0.0052, 0.008, 0.0093, 0.007, 0.0053, 0.0238, 0.006, 0.0769, 0.0102, 0.0084, 0.037, 0.0061, 0.0065, 0.0833, 0.0088, 0.0122, 0.0345, 0.0159, 0.0064, 0.006, 0.0303, 0.0137, 0.0074, 0.012, 0.0105, 0.0069, 0.0556, 0.0053, 0.0086, 0.013, 0.01, 0.0054, 0.0167, 0.0083, 0.0185, 0.1111, 0.0081, 0.0085, 0.0109, 0.0714, 0.0104, 0.0294, 0.0154, 0.0115, 0.0078, 0.0051, 0.0227, 0.0179, 0.011, 0.0909, 0.0095, 0.0071, 0.0152, 0.0074, 0.0097, 0.0056, 0.0052, 0.027, 0.0063, 0.0096, 0.0112, 0.0056, 0.1, 0.0118, 0.0055, 0.0256, 0.0055, 0.0169, 0.0111, 0.0244, 0.05, 0.0059, 0.0068, 0.007, 0.0145, 0.0088, 0.0051, 0.0135, 0.0204, 0.0156, 0.0053, 0.0065, 0.0217, 0.0071, 0.0059, 0.0057, 0.2, 0.0189, 0.009, 0.0208, 0.125, 0.0143, 0.0192, 0.0132, 0.0054, 0.0056, 0.0123, 0.0052, 0.0069, 0.0073, 0.0127, 0.0051, 0.0164, 0.0057, 0.0068, 0.0058, 0.0098, 0.0222, 0.04, 0.0058, 0.005, 0.0101, 0.0133, 0.0108, 0.0089, 0.006, 0.0435, 0.025, 0.0085, 0.0139, 0.0128, 0.0079, 0.0058, 0.0114, 0.0312, 0.0051, 0.02, 0.0077, 0.0076, 0.0063, 0.0357, 0.3333, 0.1429, 0.0064, 0.005, 0.0149, 0.0385, 0.0091, 0.0233, 0.0476, 0.0066, 0.0125, 0.0054, 0.0417, 0.0119, 0.0116, 0.0083, 0.0667, 0.0052, 0.0065, 0.0333, 0.0055, 0.0062, 0.0286]}}.\n",
      "[2024-11-11 12:05:43,543 INFO] Sample 61263 of the training set: {'query_id': '418432', 'query': 'is money awarded in a wrongful death lawsuit taxable', 'positives': {'doc_id': ['1301491'], 'score': [-1.0]}, 'negatives': {'doc_id': ['8687598', '8658095', '8681264', '8256683', '71020', '5246468', '8717471', '2889069', '8674903', '6466725', '1579337', '7289847', '544840', '6723629', '7359844', '5357215', '1985809', '4886597', '7200277', '5397360', '2019665', '4315000', '4085854', '6370381', '1491555', '5198384', '1301489', '7899919', '3303992', '5198387', '1301494', '417482', '5109510', '6697483', '4778560', '4567960', '3528661', '4398086', '399713', '2119165', '5089141', '8256682', '580207', '2878045', '1409417', '1916116', '254521', '970887', '5930636', '6792891', '5840604', '8228736', '4902760', '7184506', '3615360', '7737051', '2300517', '6037007', '476371', '7847756', '2162227', '5037264', '8649186', '1437170', '2749009', '6403212', '3821932', '1340263', '1987365', '849136', '2661287', '5246473', '2237023', '2237025', '1690246', '4094087', '4902761', '8011593', '8256680', '8244216', '1081280', '8484267', '1294390', '4658551', '3232878', '1305628', '4375953', '1498099', '3276919', '6569384', '6389780', '1703672', '1417369', '2687422', '3594035', '5198392', '8794612', '4000360', '1452093', '8687607', '2131791', '8290529', '1340270', '3417609', '154124', '5832253', '3816721', '4194260', '7889645', '8647756', '3861226', '7502483', '3881900', '2237027', '6403208', '8687605', '5149337', '2263808', '4902755', '7027500', '7502491', '3787806', '3864659', '501298', '41616', '5222171', '7170309', '1267233', '8244220', '4902762', '977099', '1776899', '6645150', '6913269', '1205473', '1141332', '5760643', '1552551', '224358', '1149680', '867108', '5319808', '1732266', '8531117', '4583965', '30074', '8611495', '2237028', '3616916', '8700882', '8472958', '2119160', '6871574', '2648253', '5992705', '1987364', '8011588', '1225769', '996735', '5992702', '7517324', '1511598', '3983151', '5930635', '2048845', '4460802', '6645145', '6645151', '3787803', '4926502', '7726640', '5152996', '1585695', '5540045', '3460994', '291455', '5975821', '1301486', '6370630', '2452028', '4234716', '7132172', '539386', '1491554', '1714534', '7502485', '1401716', '7987683', '1038407', '5930637', '6869973', '1177027', '8011590', '645476', '8044653', '3151771', '7249111', '8608149', '7508129', '578270'], 'score': [0.0063, 0.0102, 0.0091, 0.0114, 0.006, 0.009, 0.0169, 0.0152, 0.0097, 0.0769, 0.0061, 0.0065, 0.0115, 0.0714, 0.0075, 0.0159, 0.0094, 0.0105, 0.0244, 0.0192, 0.1, 0.0071, 0.0476, 0.0109, 0.0074, 0.0227, 0.0127, 0.0154, 0.0156, 0.0081, 0.0076, 0.1111, 0.007, 0.0065, 0.0054, 0.0182, 0.0056, 0.0064, 0.025, 0.0112, 0.0106, 0.005, 0.0051, 0.0078, 0.0075, 0.0088, 0.0217, 0.0072, 0.0909, 0.0072, 0.0056, 0.0101, 0.012, 0.0087, 0.0385, 0.0061, 0.0213, 0.0095, 0.0081, 0.0103, 0.01, 0.05, 0.0089, 0.0064, 0.0084, 0.0077, 0.0667, 0.0074, 1.0, 0.006, 0.0233, 0.0057, 0.0357, 0.0143, 0.0455, 0.0625, 0.02, 0.0118, 0.0204, 0.0066, 0.008, 0.0068, 0.0076, 0.0056, 0.0135, 0.0079, 0.0059, 0.0055, 0.0417, 0.0053, 0.0123, 0.013, 0.0063, 0.0052, 0.0526, 0.0175, 0.0067, 0.027, 0.0139, 0.0141, 0.0556, 0.0222, 0.0054, 0.0145, 0.0053, 0.0053, 0.0062, 0.0057, 0.0051, 0.011, 0.1429, 0.0068, 0.0085, 0.006, 0.0093, 0.0093, 0.0061, 0.04, 0.0132, 0.0833, 0.0066, 0.0286, 0.0053, 0.0055, 0.25, 0.0083, 0.0099, 0.0323, 0.0312, 0.0096, 0.125, 0.0083, 0.007, 0.0185, 0.0278, 0.0058, 0.0069, 0.0052, 0.0116, 0.0067, 0.2, 0.0058, 0.0069, 0.0108, 0.0345, 0.0238, 0.0164, 0.0063, 0.0196, 0.0179, 0.0111, 0.0161, 0.0294, 0.0054, 0.0435, 0.0172, 0.0079, 0.0085, 0.0052, 0.0333, 0.0086, 0.0147, 0.0149, 0.0065, 0.0062, 0.0167, 0.0055, 0.0051, 0.0119, 0.0303, 0.0056, 0.0092, 0.5, 0.0058, 0.0133, 0.0128, 0.0082, 0.0051, 0.0073, 0.0208, 0.0189, 0.0098, 0.0125, 0.0263, 0.1667, 0.0078, 0.0104, 0.0057, 0.0122, 0.005, 0.0256, 0.0071, 0.0088, 0.0059, 0.3333, 0.0068, 0.0052, 0.037, 0.0588, 0.0137]}}.\n",
      "[2024-11-11 12:05:43,544 INFO] Sample 3927 of the training set: {'query_id': '517074', 'query': 'the meaning of long winded conversation', 'positives': {'doc_id': ['184500'], 'score': [-1.0]}, 'negatives': {'doc_id': ['1712907', '1605035', '6944705', '5020275', '931869', '2458563', '5734516', '2929010', '3961974', '7902820', '2673828', '3217058', '8631252', '2737488', '1704688', '1951731', '3323614', '3668914', '1780100', '6440943', '897079', '157057', '6811004', '6440945', '8191370', '1307929', '1719833', '1881717', '3501761', '8188043', '4985834', '2061090', '6364524', '2439072', '2773153', '8399455', '7689724', '2686588', '7207402', '4214830', '143616', '7479987', '8666579', '4650863', '6036596', '4636485', '6440942', '3711672', '6190744', '5574250', '8471811', '2066764', '1032572', '7025779', '8502965', '7045988', '7479990', '2861072', '5169502', '3121992', '8818670', '7572263', '8040163', '1533178', '7586610', '8108786', '3476617', '4006375', '7465520', '171027', '2223421', '7969666', '5857414', '1850387', '6131673', '448561', '1203127', '7902818', '4827369', '3442471', '6834490', '5552807', '2161592', '1462425', '522455', '8561289', '8222880', '7994073', '53567', '5796838', '5765673', '8341748', '3593316', '1040473', '428197', '5492448', '919671', '8363436', '6499728', '3623205', '5484200', '3073024', '4219846', '2125473', '2441682', '296634', '1325099', '8522307', '2766268', '2694575', '184496', '8058194', '8727463', '726306', '6813064', '5553115', '8580635', '8438329', '6157166', '2639678', '4213562', '4327486', '4116455', '637804', '3435465', '786228', '4149494', '8837822', '3102954', '1136157', '3176482', '7304557', '8666582', '3170063', '5219170', '3669589', '4820633', '7366153', '2961002', '1608934', '6533224', '7433452', '5122387', '6646195', '5800026', '8818666', '8086583', '3618204', '6339547', '437519', '8030895', '114284', '7347014', '8580633', '155605', '2479842', '8561287', '1136160', '8527282', '7233286', '3926331', '8693957', '1153578', '4748445', '7868361', '8380021', '1858198', '8638029', '8791151', '8306400', '1899870', '7045989', '7179429', '3168021', '7991810', '3668908', '5697869', '5694663', '6647550', '4982907', '184499', '3473695', '7800083', '7877136', '184497', '432463', '3547035', '5925265', '3387052', '5302056', '2760796', '1926468', '4662988', '1414370', '6397612', '8841481', '4215443', '1966651', '6234520', '6284717'], 'score': [0.0081, 0.0208, 0.0333, 0.0096, 0.0053, 0.0056, 0.0161, 0.0122, 0.0244, 0.0127, 0.5, 0.0139, 0.0065, 0.0052, 0.0051, 0.0083, 0.0109, 0.0054, 0.0102, 0.0833, 0.0068, 0.007, 0.0111, 0.0089, 0.0185, 0.0182, 0.125, 0.0625, 0.0083, 0.0227, 0.007, 0.0074, 0.0116, 0.04, 0.0065, 0.0167, 0.0075, 0.0082, 0.0088, 0.0053, 0.0055, 0.0079, 0.0435, 0.25, 0.0092, 0.0312, 0.0526, 0.0137, 0.0063, 0.0091, 0.0455, 0.0055, 0.0385, 0.0067, 0.0051, 0.1111, 0.0065, 0.0056, 0.0072, 0.0084, 0.0058, 0.0057, 0.0135, 0.0088, 0.0115, 0.0061, 0.0172, 0.006, 0.0086, 1.0, 0.0058, 0.0345, 0.0095, 0.0079, 0.0104, 0.0145, 0.0204, 0.0052, 0.0357, 0.0667, 0.0128, 0.0222, 0.027, 0.05, 0.0196, 0.0101, 0.0263, 0.0179, 0.0068, 0.0323, 0.0078, 0.005, 0.0192, 0.0714, 0.0213, 0.0217, 0.0156, 0.0154, 0.0119, 0.0073, 0.0059, 0.0476, 0.1429, 0.037, 0.0093, 0.2, 0.0085, 0.025, 0.0159, 0.0051, 0.0052, 0.005, 0.0056, 0.0286, 0.3333, 0.0076, 0.0097, 0.0108, 0.0066, 0.1667, 0.0051, 0.006, 0.0098, 0.02, 0.0909, 0.0303, 0.0588, 0.0143, 0.01, 0.0062, 0.0112, 0.0556, 0.0069, 0.0147, 0.0066, 0.0055, 0.0164, 0.013, 0.0062, 0.0132, 0.0152, 0.009, 0.0054, 0.0175, 0.0125, 0.0058, 0.0769, 0.0105, 0.0133, 0.0052, 0.0071, 0.0061, 0.0256, 0.0077, 0.0053, 0.0063, 0.0072, 0.0118, 0.0053, 0.0094, 0.0074, 0.0057, 0.0093, 0.0087, 0.0059, 0.0099, 0.0278, 0.1, 0.012, 0.0061, 0.0054, 0.0123, 0.011, 0.006, 0.0064, 0.0064, 0.0106, 0.0189, 0.0067, 0.0071, 0.0075, 0.0063, 0.0417, 0.0169, 0.0081, 0.0294, 0.008, 0.0233, 0.0141, 0.0103, 0.0076, 0.0069, 0.0068, 0.0085, 0.0238, 0.0078, 0.0149, 0.0056, 0.0114, 0.0057]}}.\n",
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_data_loader = RetrievalDataLoader(args=args, tokenizer=tokenizer)\n",
    "train_dataset = retrieval_data_loader.train_dataset\n",
    "eval_dataset = retrieval_data_loader.eval_dataset\n",
    "\n",
    "trainer: Trainer = BiencoderTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset if args.do_train else None,\n",
    "    eval_dataset=eval_dataset if args.do_eval else None,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial(_compute_metrics, args),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.remove_callback(PrinterCallback)\n",
    "trainer.add_callback(LoggerCallback)\n",
    "retrieval_data_loader.trainer = trainer\n",
    "model.trainer = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_input_ids',\n",
       " 'q_token_type_ids',\n",
       " 'q_attention_mask',\n",
       " 'd_input_ids',\n",
       " 'd_token_type_ids',\n",
       " 'd_attention_mask']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "list(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"q_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1007,\n",
       " 2054,\n",
       " 2001,\n",
       " 1996,\n",
       " 6234,\n",
       " 4254,\n",
       " 1997,\n",
       " 1996,\n",
       " 3112,\n",
       " 1997,\n",
       " 1996,\n",
       " 7128,\n",
       " 2622,\n",
       " 1029,\n",
       " 102]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"q_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"d_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57, 59, 47, 98, 44, 79, 70, 89, 71, 63, 72, 76, 46, 57, 55, 32]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(d) for d in example[\"d_input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ) what was the immediate impact of the success of the manhattan project? [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/simlm-base-msmarco\")\n",
    "tokenizer.decode(example[\"q_input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] history of the twin towers [SEP] downtown lower manhattan association is created by real estate developer david rockefeller to revitalize lower manhattan and begins to promote the idea of a world trade and finance center in new york city. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"d_input_ids\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": trainer._train_batch_size,\n",
    "    \"collate_fn\": data_collator,\n",
    "    \"num_workers\": trainer.args.dataloader_num_workers,\n",
    "    \"pin_memory\": trainer.args.dataloader_pin_memory,\n",
    "    \"persistent_workers\": trainer.args.dataloader_persistent_workers,\n",
    "}\n",
    "\n",
    "if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "    dataloader_params[\"sampler\"] = trainer._get_train_sampler()\n",
    "    dataloader_params[\"drop_last\"] = trainer.args.dataloader_drop_last\n",
    "    dataloader_params[\"prefetch_factor\"] = trainer.args.dataloader_prefetch_factor\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:314] 2024-11-11 12:05:44,663 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_input_ids': tensor([[  101,  4248, 17470,  4013,  2490,  2193,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2003,  1996,  2783,  2051, 22851,  2102,  2030,  8827,  2102,\n",
       "           102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2515,  2632,  8569, 27833,  2812,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2043,  2064,  2017,  4929,  2317,  6471,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  6207, 11306,  1005,  1055,  5592,  4003,  5703,  3042,  2193,\n",
       "           102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2064,  1045,  6570,  6001,  2007, 13908, 13597,   102,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2079, 21122, 15580,  9880,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2217,  2003,  2115, 22524,  2006,  2005,  2273,  1029,\n",
       "           102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2129,  2411,  2323,  1045,  2689,  1055, 26148,  6943,  4641,\n",
       "           102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  1996,  5628,  1997,  1996,  3618, 20066, 10222,\n",
       "          3560, 12818,   102,     0,     0,     0],\n",
       "        [  101,  2040,  2003,  5192,  5965,  4783,  2906,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2029, 12335,  1997,  1996,  4231,  2064,  2017,  2156,  1999,\n",
       "          1996,  2851,   102,     0,     0,     0],\n",
       "        [  101,  2054,  4319,  2465,  2003,  2061,  2863,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2003, 11942, 10024,  3489,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  1037, 18856,  9581, 23479,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2079,  2017, 10768, 28228,  3669,  4371, 10958, 10521, 15689,\n",
       "           102,     0,     0,     0,     0,     0]]), 'q_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'q_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), 'd_input_ids': tensor([[  101,  4248, 17470,  ...,     0,     0,     0],\n",
       "        [  101,  1011,   102,  ...,     0,     0,     0],\n",
       "        [  101,  4248, 17470,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1011,   102,  ...,  1996,  3302,   102],\n",
       "        [  101,  1996, 14266,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  2079,  ...,     0,     0,     0]]), 'd_token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'd_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"q_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4248, 17470,  4013,  2490,  2193,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2003,  1996,  2783,  2051, 22851,  2102,  2030,  8827,  2102,\n",
       "            102,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2515,  2632,  8569, 27833,  2812,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2043,  2064,  2017,  4929,  2317,  6471,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  6207, 11306,  1005,  1055,  5592,  4003,  5703,  3042,  2193,\n",
       "            102,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2064,  1045,  6570,  6001,  2007, 13908, 13597,   102,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2079, 21122, 15580,  9880,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2217,  2003,  2115, 22524,  2006,  2005,  2273,  1029,\n",
       "            102,     0,     0,     0,     0,     0],\n",
       "         [  101,  2129,  2411,  2323,  1045,  2689,  1055, 26148,  6943,  4641,\n",
       "            102,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2024,  1996,  5628,  1997,  1996,  3618, 20066, 10222,\n",
       "           3560, 12818,   102,     0,     0,     0],\n",
       "         [  101,  2040,  2003,  5192,  5965,  4783,  2906,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2029, 12335,  1997,  1996,  4231,  2064,  2017,  2156,  1999,\n",
       "           1996,  2851,   102,     0,     0,     0],\n",
       "         [  101,  2054,  4319,  2465,  2003,  2061,  2863,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2003, 11942, 10024,  3489,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  2003,  1037, 18856,  9581, 23479,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2079,  2017, 10768, 28228,  3669,  4371, 10958, 10521, 15689,\n",
       "            102,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trainers.biencoder_trainer import _unpack_qp\n",
    "query_batch_dict, doc_batch_dict = _unpack_qp(batch)\n",
    "query_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "def init_distributed_single_gpu():\n",
    "    # Set environment variables\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    \n",
    "    # Initialize process group\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",  # Use NCCL backend for GPU\n",
    "        rank=0,          # Single GPU, so rank is 0\n",
    "        world_size=1     # Total number of processes is 1\n",
    "    )\n",
    "    \n",
    "    # Set the device\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "init_distributed_single_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "query_batch_dict = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in query_batch_dict.items()}\n",
    "doc_batch_dict = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in doc_batch_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(query=query_batch_dict, passage=doc_batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiencoderOutput(q_reps=tensor([[ 0.0355, -0.0662,  0.0463,  ..., -0.0304,  0.0158,  0.0318],\n",
       "        [ 0.0407, -0.0098,  0.0277,  ...,  0.0115,  0.0487, -0.0128],\n",
       "        [ 0.0358,  0.0359,  0.0065,  ..., -0.0172, -0.0137, -0.0005],\n",
       "        ...,\n",
       "        [ 0.0457,  0.0100, -0.0348,  ..., -0.0243,  0.0192, -0.0326],\n",
       "        [-0.0131,  0.0027, -0.0103,  ..., -0.0231,  0.0251,  0.0269],\n",
       "        [ 0.0325,  0.0376, -0.0209,  ..., -0.0306, -0.0295, -0.0003]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>), p_reps=tensor([[-2.3926e-02,  2.9929e-04,  1.7268e-02,  ...,  1.2327e-03,\n",
       "          3.0578e-03,  4.8459e-02],\n",
       "        [ 4.3832e-02,  6.3219e-03,  1.8771e-03,  ...,  2.9059e-02,\n",
       "          2.4033e-02,  1.9232e-02],\n",
       "        [ 1.2833e-02, -2.8508e-02, -2.6362e-02,  ...,  3.7568e-02,\n",
       "         -2.4967e-03,  1.3990e-03],\n",
       "        ...,\n",
       "        [ 2.5597e-02, -1.2830e-02, -6.7153e-05,  ...,  2.8175e-02,\n",
       "          1.7792e-02,  1.4907e-02],\n",
       "        [ 1.8049e-02,  3.0032e-02,  8.6424e-03,  ...,  2.8988e-02,\n",
       "         -1.4922e-02,  2.3679e-02],\n",
       "        [ 3.1937e-02,  1.9218e-02,  1.6095e-02,  ...,  1.1006e-02,\n",
       "          7.2254e-03, -1.6836e-02]], device='cuda:0', grad_fn=<DivBackward0>), loss=tensor(10.0293, device='cuda:0', grad_fn=<MulBackward0>), labels=tensor([  0,  16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
       "        224, 240], device='cuda:0'), scores=tensor([[25.4375, 16.5557, 20.2534,  ..., 11.3600, 13.9457, 11.8904],\n",
       "        [16.3859, 11.6685, 13.0921,  ..., 10.3353, 11.4774, 12.7052],\n",
       "        [10.0966, 11.5806, 12.8352,  ..., 12.1476, 14.6538, 13.5622],\n",
       "        ...,\n",
       "        [11.8613, 13.9506, 12.9322,  ..., 10.2441, 13.9579, 14.0733],\n",
       "        [14.5899, 14.0012, 14.8967,  ..., 12.9716, 13.4374, 10.8288],\n",
       "        [10.3964, 12.4465, 13.5161,  ..., 14.5838, 18.9146, 16.5571]],\n",
       "       device='cuda:0', grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0293, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.epoch = 0\n",
    "trainer.compute_loss(model, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
