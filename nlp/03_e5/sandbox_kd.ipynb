{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/repos/unilm/simlm/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/traindata/maksim/repos/unilm/simlm/src\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from typing import Dict\n",
    "from functools import partial\n",
    "from transformers.utils.logging import enable_explicit_format\n",
    "from transformers.trainer_callback import PrinterCallback\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    PreTrainedTokenizerFast\n",
    ")\n",
    "\n",
    "from logger_config import logger, LoggerCallback\n",
    "from config import Arguments\n",
    "from trainers import BiencoderTrainer\n",
    "from loaders import RetrievalDataLoader\n",
    "from collators import BiencoderCollator\n",
    "from metrics import accuracy, batch_mrr\n",
    "from models import BiencoderModel\n",
    "\n",
    "def _common_setup(args: Arguments):\n",
    "    if args.process_index > 0:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    enable_explicit_format()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "\n",
    "def _compute_metrics(args: Arguments, eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    # field consistent with BiencoderOutput\n",
    "    preds = eval_pred.predictions\n",
    "    scores = torch.tensor(preds[-1]).float()\n",
    "    labels = torch.arange(0, scores.shape[0], dtype=torch.long) * args.train_n_passages\n",
    "    labels = labels % scores.shape[1]\n",
    "\n",
    "    topk_metrics = accuracy(output=scores, target=labels, topk=(1, 3))\n",
    "    mrr = batch_mrr(output=scores, target=labels)\n",
    "\n",
    "    return {'mrr': mrr, 'acc1': topk_metrics[0], 'acc3': topk_metrics[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_DIR\"] = \"./data/msmarco_bm25_official/\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"./tmp/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arguments(output_dir='/traindata/maksim/repos/unilm/simlm/checkpoint/distilled_biencoder/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=1000, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/traindata/maksim/repos/unilm/simlm/checkpoint/distilled_biencoder/runs/Nov12_13-29-46_ip-172-19-213-218', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=500, save_total_limit=10, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=123, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, dataloader_prefetch_factor=None, past_index=-1, run_name='/traindata/maksim/repos/unilm/simlm/checkpoint/distilled_biencoder/', disable_tqdm=True, remove_unused_columns=False, label_names=['labels'], load_best_model_at_end=True, metric_for_best_model='mrr', greater_is_better=True, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, gradient_accumulation_kwargs=None), deepspeed='/traindata/maksim/repos/unilm/simlm/ds_config.json', label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, model_name_or_path='intfloat/simlm-base-msmarco', data_dir='/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation/', task_type='ir', train_file='/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation//kd_train.jsonl', validation_file='/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation//kd_dev.jsonl', train_n_passages=24, share_encoder=True, use_first_positive=False, use_scaled_loss=True, loss_scale=-1.0, add_pooler=False, out_dimension=768, t=0.02, l2_normalize=True, t_warmup=False, full_contrastive_loss=True, do_encode=False, encode_in_path=None, encode_save_dir=None, encode_shard_size=2000000, encode_batch_size=256, do_search=False, search_split='dev', search_batch_size=128, search_topk=200, search_out_dir='', do_rerank=False, rerank_max_length=256, rerank_in_path='', rerank_out_path='', rerank_split='dev', rerank_batch_size=128, rerank_depth=1000, rerank_forward_factor=1, rerank_use_rdrop=False, do_kd_gen_score=False, kd_gen_score_split='dev', kd_gen_score_batch_size=128, kd_gen_score_n_neg=30, do_kd_biencoder=True, kd_mask_hn=False, kd_cont_loss_weight=0.2, rlm_generator_model_name='google/electra-base-generator', rlm_freeze_generator=True, rlm_generator_mlm_weight=0.2, all_use_mask_token=False, rlm_num_eval_samples=4096, rlm_max_length=144, rlm_decoder_layers=2, rlm_encoder_mask_prob=0.3, rlm_decoder_mask_prob=0.5, q_max_len=32, p_max_len=144, max_train_samples=None, dry_run=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['src/train_biencoder.py', '--deepspeed', '/traindata/maksim/repos/unilm/simlm/ds_config.json', '--model_name_or_path', 'intfloat/simlm-base-msmarco', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--kd_mask_hn', 'False', '--kd_cont_loss_weight', '0.2', '--seed', '123', '--do_train', '--do_kd_biencoder', '--t', '0.02', '--fp16', '--train_file', '/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation//kd_train.jsonl', '--validation_file', '/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation//kd_dev.jsonl', '--q_max_len', '32', '--p_max_len', '144', '--train_n_passages', '24', '--dataloader_num_workers', '1', '--num_train_epochs', '6', '--learning_rate', '3e-5', '--warmup_steps', '1000', '--share_encoder', 'True', '--logging_steps', '50', '--output_dir', '/traindata/maksim/repos/unilm/simlm/checkpoint/distilled_biencoder/', '--data_dir', '/traindata/maksim/repos/unilm/simlm/data/msmarco_distillation/', '--save_total_limit', '10', '--save_strategy', 'epoch', '--evaluation_strategy', 'epoch', '--load_best_model_at_end', '--metric_for_best_model', 'mrr', '--greater_is_better', 'True', '--remove_unused_columns', 'False', '--overwrite_output_dir', '--disable_tqdm', 'True', '--report_to', 'none']\n",
    "parser = HfArgumentParser((Arguments,))\n",
    "args: Arguments = parser.parse_args_into_dataclasses()[0]\n",
    "_common_setup(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[WARNING|modeling_utils.py:4172] 2024-11-12 13:29:47,547 >> Some weights of BertModel were not initialized from the model checkpoint at intfloat/simlm-base-msmarco and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-11-12 13:29:47,576 INFO] BiencoderModel(\n",
      "  (lm_q): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lm_p): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cross_entropy): CrossEntropyLoss()\n",
      "  (kl_loss_fn): KLDivLoss()\n",
      "  (pooler): Identity()\n",
      ")\n",
      "[2024-11-12 13:29:47,583 INFO] Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "model: BiencoderModel = BiencoderModel.build(args=args)\n",
    "logger.info(model)\n",
    "logger.info('Vocab size: {}'.format(len(tokenizer)))\n",
    "\n",
    "data_collator = BiencoderCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if args.fp16 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-12 13:29:47,829 INFO] Sample 27453 of the training set: {'query_id': '689043', 'query': 'what is a lime rickey?', 'positives': {'doc_id': ['559262'], 'score': [2.54883]}, 'negatives': {'doc_id': ['945415', '2225798', '2174436', '4025338', '1351518', '189410', '3841208', '559263', '5637829', '4967760', '4070900', '3476454', '559258', '3799440', '5035615', '5055167', '7631620', '3030057', '8075849', '7609175', '559257', '4810917', '3653953', '51826', '945414', '4967767', '7003968', '559259', '4038251', '4573199', '3991139', '559264', '4057818', '7157577', '3438664', '3991141', '1875093', '1378439', '8123451', '7610009', '2653049', '5984731', '3991140', '6513406', '2526329', '2560919', '5018273', '1378438', '8410762', '7631615', '3369864', '7631617', '5393019', '4718163', '6513415', '2526334', '1555993', '7743876', '3007692', '3081016', '4070908', '5344416', '2437633', '1924078', '3897853', '3799438', '7612325', '4573192', '2710018', '2936717', '2936722', '1379731', '3297383', '52529', '2817489', '4389331', '6748981', '559256', '4389327', '1877285', '4115465', '1378433', '6835167', '4573195', '7743873', '6638018', '3072988', '3890856', '2925822', '2710019', '4070907', '6835161', '1040812', '2174441', '4724831', '2150598', '6769304', '1340081', '655055', '7666743', '4070904', '7003961', '7520496', '975923', '4573198', '8322633', '7327315', '4480241', '4810920', '5984726', '5526268', '4740223', '5331780', '945417', '7157571', '6513413', '7615990', '7612322', '2150596', '2526328', '4967766', '2925823', '257310', '2505958', '3234188', '52524', '3991137', '3081013', '7926801', '945413', '6638016', '8375560', '2767196', '8677201', '2550513', '6949478', '7631616', '2325469', '4038252', '5393017', '3799437', '1340087', '4810919', '4758419', '2817490', '2174437', '2773510', '3632383', '2150599', '559261', '6888275', '5882025', '3339814', '4070899', '2773509', '4573196', '2741549', '3525337', '1379728', '5984732', '1079064', '7225211', '3991142', '2924060', '2874553', '559260', '4389333', '8289603', '2526337', '5344421', '4652982', '7648755', '1351520', '7820773', '7666744', '8442489', '52528', '2015831', '7810254', '1028461', '945416', '5426828', '1615425', '4931956', '7584422', '2181701', '1499381', '4389328', '1615422', '4573200', '1379732', '4038253', '1380936', '3234192', '2150590', '4389330', '4049715', '1571105', '4389326', '611012', '806410', '7223754', '3230040', '559255', '945418', '4038249', '8078839', '4371551', '4970570', '2533576'], 'score': [-7.01953, -7.77734, -7.26172, -7.32812, -7.34766, -7.05078, -7.74609, 1.33984, -6.81641, -7.33984, -6.85156, -7.04297, 1.35254, -7.19141, -7.17188, -7.28125, -6.84766, -6.80469, -7.1875, -6.65234, -2.24414, -7.08594, -7.85547, -7.26172, -7.3125, -7.36719, -7.63672, -3.80078, -6.98047, -7.29688, -3.82227, -0.98828, -4.49219, -7.01172, -7.57422, -6.02734, -7.25, -7.23047, -7.36719, -7.83594, -7.31641, -7.32812, -7.35938, 0.33569, -6.98047, -7.17188, -7.22266, -6.9375, -7.54688, -7.12109, -6.24219, -7.07812, -7.37891, -7.03906, -2.75586, -7.45703, -7.13281, -7.14844, -6.89453, -7.16406, -7.26953, -7.27344, -7.375, -7.19922, -7.64062, -7.09375, -7.25391, -6.96484, -7.39453, -7.23828, -7.21875, -7.02734, -7.72266, -7.53516, -6.47266, -7.22656, -7.25, -2.24805, -7.02344, -7.55469, -7.42188, -7.41797, -7.35938, -7.39844, -7.15234, -7.52344, -7.34375, -6.83203, -7.41797, -7.63281, -7.11328, -7.11328, -7.44922, -7.34766, -6.93359, -7.51172, -7.25391, -7.34766, -7.375, -7.35547, -7.09766, -7.59766, -7.50781, -4.26953, -7.26562, -6.94922, -7.58203, -7.45703, -7.26562, -7.22266, -7.63281, -6.91406, -7.80078, -6.8125, -6.96094, -2.24414, -7.19141, -7.05469, -7.62891, -7.25391, -7.21484, -7.35547, -7.35938, -7.14062, -7.67578, -7.25781, -5.71875, -6.84766, -6.53125, -6.71875, -6.96484, -7.75781, -6.67578, -7.60156, -7.66797, -2.24805, -7.08984, -7.36328, -6.42188, -7.31641, -7.53125, -7.5, -7.10938, -7.42188, -7.12109, -7.33984, -7.07422, -7.35938, -7.64062, -2.48438, -7.42188, -7.76562, -7.57031, -6.85938, -7.31641, -6.95312, -6.97266, -7.90625, -7.23438, -7.68359, -7.61328, -6.79297, -7.21875, -6.92578, -7.28516, -6.46875, -7.06641, -7.33984, -7.37109, -7.47266, -7.05469, -7.37891, -7.60156, -7.81641, -7.42578, -6.22266, -6.69141, -7.76953, -7.17578, -7.69922, -6.66797, -7.73828, -6.99609, -7.08594, -7.33594, -7.57031, -7.76172, -7.08984, -6.89453, -7.22656, -7.32422, -6.39453, -7.07422, -7.53516, -7.24219, -7.39062, -6.85938, -7.85938, -7.25391, -6.66797, -7.26953, -6.91016, -7.75, -6.08594, -6.67188, -7.40625, -7.43359, -7.67188, -7.72656, -1.6582]}}.\n",
      "[2024-11-12 13:29:47,831 INFO] Sample 140339 of the training set: {'query_id': '38702', 'query': 'average male weight', 'positives': {'doc_id': ['781436'], 'score': [1.33008]}, 'negatives': {'doc_id': ['3242992', '1520385', '249484', '8286800', '5508305', '5705998', '5230340', '5955169', '7351517', '3627863', '4044277', '4501247', '6182949', '11770', '3627864', '6545793', '1740293', '3540906', '324908', '6966912', '5905216', '722564', '5350963', '7407405', '3139774', '8724410', '1451327', '5507032', '3262444', '7407399', '6115920', '5529447', '7360265', '6115914', '2693523', '5600881', '6545785', '11768', '2189934', '174657', '1467324', '5498249', '6600169', '115449', '998288', '7679683', '6284478', '2284159', '911815', '5617290', '6350416', '4460409', '3540911', '3540905', '5930061', '6900863', '1951645', '8724412', '2469206', '2855078', '5498254', '23633', '3534585', '6186901', '6236059', '3565649', '1529646', '7509732', '1269059', '4518507', '6452936', '6966914', '3762312', '4542185', '7969816', '324906', '1979193', '1811042', '1811041', '3097452', '1451325', '6004398', '4061049', '998285', '4999924', '4495491', '3565648', '3037120', '1392361', '6352025', '715114', '6352021', '5678080', '715120', '3540907', '8786790', '3480103', '6204291', '1131964', '2270776', '1811039', '5723280', '6320387', '1740296', '3336118', '4495498', '82033', '7679681', '8801409', '5266515', '6809069', '5930062', '2076919', '5678079', '3480105', '7904868', '7407401', '5826540', '6352022', '7946454', '6115918', '3540908', '5608406', '1227686', '2076915', '3268500', '8286797', '2638454', '5350962', '2702361', '3842579', '7679682', '6404750', '7895069', '801509', '6182950', '8813352', '8801417', '615823', '6350421', '781437', '2114814', '2076916', '3139775', '615822', '1529644', '3727501', '3540910', '6135705', '2118126', '3480102', '6352020', '2469208', '3058014', '2270774', '2495254', '23642', '5905220', '3139771', '3242997', '6051179', '3540904', '5796539', '4501246', '6871139', '1529639', '7352845', '7407403', '3480100', '1349007', '6966910', '4133110', '6352019', '2219608', '3097455', '3139772', '160100', '5507034', '5678086', '6966911', '7895071', '46895', '174663', '184741', '3625921', '6687163', '615825', '8607472', '7891406', '5507033', '3540903', '8188749', '3540909', '6687166', '3336117', '6086910', '6940702', '5369169', '4044282', '5617284', '4542179', '6352026', '8813351', '6525863', '998286', '88616', '8736610', '6182946', '6320391', '1725740'], 'score': [-0.26489, -0.23877, -0.5708, -0.58545, -0.72803, -0.39893, -0.83057, -0.71729, -0.50928, -0.64111, -0.55225, -0.53711, -0.44189, -0.55371, -0.59766, 1.68164, -0.37256, -0.7627, -0.49512, -0.60693, -0.35718, 0.08154, -0.38452, -0.42358, -0.39282, -0.46045, -1.14355, -0.47388, -0.19409, -0.3855, -0.34497, -0.47437, 0.26489, -0.73633, -0.75928, -0.35327, -0.30542, -0.18298, -5.63672, -0.71533, -1.20312, 0.48364, -0.75879, -1.00098, -0.87451, -0.98193, -0.22534, -6.51953, -0.99414, -0.94824, -0.51904, -0.95459, -0.95215, -0.26611, 0.32568, -0.59814, -1.9248, -0.4165, 0.01617, -6.76172, -0.74902, -1.12109, -0.38525, -0.12512, -0.87939, -0.33252, -0.50342, 0.23022, -1.04004, -0.56934, -0.47485, -0.71924, -0.63623, -0.729, -0.79102, -0.31079, -0.33472, -0.33667, -0.54492, -0.74316, -1.31934, -0.86133, -0.47046, -0.75195, -2.37305, -0.88477, 0.03091, -0.60205, -0.0874, -0.54395, -0.59473, -0.77637, -0.0733, 0.16431, -1.25684, -0.63916, 0.27295, -5.51953, -0.8501, 0.33154, -0.52832, -0.36743, -0.65088, -0.36353, -0.05875, -0.92578, -0.78613, -0.65869, -0.03705, -0.79248, -0.24255, 0.12744, -0.98096, -0.01955, -0.60938, -1.74609, -0.4646, -0.87305, -0.96973, -0.71631, -0.39307, -0.36401, -0.46631, -5.23828, -0.45898, 0.12549, -0.53516, -0.45068, -0.23584, -0.71387, -0.64258, -0.44922, -0.61719, -0.68408, -0.77148, -0.24878, -0.60986, -0.69531, 0.38989, -0.65918, -0.52393, -0.521, -0.45312, 0.3667, -0.21069, -0.01569, -0.37207, -0.38159, -1.05078, -0.42432, -0.32422, -0.54443, -0.56299, -5.58594, -0.87549, -0.72119, -0.26562, -0.33008, -0.29541, -0.55176, -0.4624, -0.26807, -0.81738, -0.10394, -6.80859, -0.52734, -0.68408, -0.45996, -0.28809, -0.59424, -0.33472, -0.47144, -1.06543, -1.0957, -0.56934, -0.48413, 0.43188, -0.46973, -0.46436, -0.22119, -0.38184, -6.23828, 0.22021, -1.58594, 0.05515, -0.61182, -0.76807, -0.20044, -5.66406, -0.18774, -0.58154, -5.82422, -0.85986, -0.84082, -0.43213, -0.57666, -0.70898, -0.36255, -0.39038, 0.45239, -0.62305, -0.77979, -0.66602, -0.57617, -0.58984, -1.06543, -0.49316, -0.27026, -0.70605, -6.5]}}.\n",
      "[2024-11-12 13:29:47,831 INFO] Sample 45710 of the training set: {'query_id': '651395', 'query': 'what does the name festus mean', 'positives': {'doc_id': ['856117'], 'score': [3.89258]}, 'negatives': {'doc_id': ['309832', '821820', '325255', '7989860', '7612230', '4156872', '1556203', '5874857', '2013463', '368500', '8392959', '323244', '1090648', '5537462', '1687584', '1556200', '295901', '2300290', '1090650', '8741052', '856120', '6754400', '5098602', '8824640', '2013466', '856119', '6688248', '1853145', '8085108', '4578226', '856118', '6183851', '6601504', '1968621', '3802943', '4539600', '1106583', '4892102', '1106581', '1579431', '6081160', '6688241', '5943013', '387176', '3744044', '7441279', '1687587', '1043342', '7704512', '4156873', '5098600', '8014731', '592577', '8550301', '6196452', '8392957', '2300284', '6035146', '573964', '6196455', '5874860', '2800156', '7441283', '7546602', '1090646', '6247888', '4573621', '7146187', '3209337', '5537459', '7017971', '8014729', '6601510', '1968619', '8824634', '1308392', '387180', '2013465', '5098604', '1090642', '6601511', '4156879', '2968589', '6196453', '5019960', '1853142', '6440950', '6247892', '7273589', '5874858', '7144675', '6625007', '4073870', '7017967', '1556202', '212452', '3956167', '6215731', '2013464', '2800155', '8085109', '8815718', '6055897', '8824641', '5033639', '1991092', '4073873', '8436044', '1106578', '3744046', '856124', '4242000', '1530971', '7570753', '387183', '2744733', '5715089', '1022347', '8328536', '4073879', '1106585', '5537461', '6688242', '1556201', '1157476', '1706329', '6183854', '6036553', '5715091', '7143546', '1853143', '388065', '1022353', '4097411', '6440949', '8824637', '7577476', '4757662', '5134109', '1706332', '4242006', '1687585', '7017972', '4156875', '4097415', '1257928', '856123', '573960', '2221593', '2800157', '3695194', '856122', '5098607', '897857', '1530972', '5420131', '1672278', '1853144', '5098601', '4688221', '7404025', '6247894', '1853146', '5873836', '5874862', '4789009', '4403958', '368499', '451140', '7404021', '6247889', '6208635', '8462166', '2415671', '923345', '5098603', '1556198', '5917672', '8824639', '8824635', '918150', '540555', '220973', '2744739', '2013467', '325253', '1451974', '3956166', '856121', '2744736', '2690185', '8085107', '1090644', '1157478', '5098599', '2556931', '7143548', '7404023', '1968620', '7404022', '3967846', '6871413', '5869431', '1022354', '7017968', '5874859', '4156878', '6379530', '2221598', '220971'], 'score': [-6.66016, -7.47656, -5.68359, -5.39844, -5.86719, -2.24219, -5.35938, -5.39844, -5.82812, -5.16797, -5.62891, -5.60938, -5.27734, -5.34766, -5.19141, -5.96484, -5.07031, -5.25, -5.76562, -6.52344, 3.08594, -4.88672, -4.79688, -5.07812, -4.4375, -3.52344, -4.75781, -5.40234, -5.74609, -5.8125, -2.07617, -5.72266, -5.75, -5.49609, -5.20312, -5.49219, -6.0625, -7.30078, -5.30469, -4.92188, -5.3125, -5.32812, -7.4375, -6.85156, -6.73438, -6.09375, -5.6875, -6.58594, -5.53516, -6.26562, -4.71875, -6.10547, -5.52734, -5.93359, -5.05078, -4.80078, -5.27734, -5.55859, -5.16406, -5.20312, -4.97656, -5.85156, -5.37109, -5.91797, -5.3125, -5.58203, -5.94141, -5.19531, -5.70312, -5.10156, -5.48047, -6.39453, -5.58594, -5.26172, -5.10156, -6.71875, -6.83594, -4.82031, -5.91406, -5.01562, -5.85156, -2.88867, -5.5625, -5.10156, -7.35156, -5.51953, -4.88281, -6.6875, -5.52344, -4.45312, -5.60547, -6.23438, -5.61719, -5.04297, -5.64062, -5.82031, -5.66016, -5.80469, -5.76172, -5.72266, -5.63672, -5.79297, -5.85547, -5.65234, -7.03125, -5.29297, -5.58203, -5.29297, -5.48047, -7.19922, 4.00391, -5.0625, -4.76172, -5.42969, -6.57031, -4.44922, -5.10938, -5.19141, -5.89062, -5.89844, -5.28125, -5.68359, -4.25, -5.91016, -5.61328, -5.83984, -5.45703, -5.65625, -5.12109, -5.24219, -5.53516, -5.29688, -5.71094, -5.67578, -5.20312, -4.92188, -5.91797, -5.49609, -5.23047, -5.96875, -5.28516, -5.46875, -4.82422, -4.24609, -5.64062, -5.85938, -4.19922, -5.51953, -5.80078, -5.72656, -7.45312, -0.31396, -5.11719, -5.92969, -4.94922, -5.26562, -5.57812, -5.33984, -5.97266, -5.58984, -5.50391, -6.69922, -5.40234, -5.65625, -4.23047, -7.33984, -5.82812, -5.0, -6.63672, -5.48047, -6.06641, -6.13281, -6.77734, -6.45703, -5.80469, -5.46484, -5.12109, -5.75391, -5.84766, -5.28906, -5.42969, -5.75, -4.65625, -6.23438, -4.34766, -5.32812, -5.0, -5.58984, -2.27344, -4.3125, -5.49609, -5.44141, -5.16406, -5.75, -4.78125, -5.58594, -4.89062, -5.46875, -5.57031, -5.32422, -7.71875, -6.125, -5.85547, -5.35547, -4.31641, -2.98047, -4.49219, -7.29297, -6.11328, -5.67188]}}.\n",
      "/traindata/maksim/miniconda3/envs/e5/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_data_loader = RetrievalDataLoader(args=args, tokenizer=tokenizer)\n",
    "train_dataset = retrieval_data_loader.train_dataset\n",
    "eval_dataset = retrieval_data_loader.eval_dataset\n",
    "\n",
    "trainer: Trainer = BiencoderTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset if args.do_train else None,\n",
    "    eval_dataset=eval_dataset if args.do_eval else None,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial(_compute_metrics, args),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.remove_callback(PrinterCallback)\n",
    "trainer.add_callback(LoggerCallback)\n",
    "retrieval_data_loader.trainer = trainer\n",
    "model.trainer = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_input_ids',\n",
       " 'q_token_type_ids',\n",
       " 'q_attention_mask',\n",
       " 'd_input_ids',\n",
       " 'd_token_type_ids',\n",
       " 'd_attention_mask',\n",
       " 'kd_labels']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "list(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"kd_labels\"]), len(example[\"d_input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57,\n",
       " 144,\n",
       " 57,\n",
       " 77,\n",
       " 83,\n",
       " 120,\n",
       " 47,\n",
       " 70,\n",
       " 65,\n",
       " 75,\n",
       " 58,\n",
       " 116,\n",
       " 130,\n",
       " 60,\n",
       " 79,\n",
       " 119,\n",
       " 47,\n",
       " 134,\n",
       " 116,\n",
       " 85,\n",
       " 84,\n",
       " 56,\n",
       " 144,\n",
       " 132]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(elem) for elem in example[\"d_input_ids\"] if elem is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.2793,\n",
       " -4.66406,\n",
       " -4.52734,\n",
       " -6.86719,\n",
       " -4.89062,\n",
       " -6.89844,\n",
       " -3.67969,\n",
       " -4.18359,\n",
       " -4.62891,\n",
       " -3.20312,\n",
       " -3.77539,\n",
       " -6.39844,\n",
       " -4.60547,\n",
       " -4.36719,\n",
       " -6.19531,\n",
       " -6.30078,\n",
       " -5.84375,\n",
       " -4.19922,\n",
       " -3.94141,\n",
       " -4.52734,\n",
       " -5.3125,\n",
       " -4.17188,\n",
       " -4.52734,\n",
       " -4.73438]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"kd_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"q_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ) what was the immediate impact of the success of the manhattan project? [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/simlm-base-msmarco\")\n",
    "tokenizer.decode(example[\"q_input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] introduction [SEP] the presence of communication amid scientific minds was equally important to the success of the manhattan project as scientific intellect was. the only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant ; hundreds of thousands of innocent lives obliterated. [SEP]\n",
      "[CLS] 51f. the manhattan project [SEP] by the summer of 1945, oppenheimer was ready to test the first bomb. on july 16, 1945, at trinity site near alamogordo, new mexico, scientists of the manhattan project readied themselves to watch the detonation of the world's first atomic bomb. the device was affixed to a 100 - foot tower and discharged just before dawn. he main assembly plant was built at los alamos, new mexico. robert oppenheimer was put in charge of putting the pieces together at los alamos. after the final bill was tallied, nearly $ 2 billion had been spent on research and development of the atomic bomb. the manhattan project employed over [SEP]\n",
      "[CLS] monroe doctrine [SEP] the immediate impact of the monroe doctrine was mixed. it was successful to the extent that the continental powers did not attempt to revive the spanish empire, but this was on account of the strength of the british navy, not american military might, which was relatively limited. [SEP]\n",
      "[CLS] - [SEP] at the end of the unit the children will be quizzed on the different senses which will have to be matched to their source. the children will have to name the five senses and which part of the body is linked with that particular sense. also, the story incorporating the five senses will also be used as a way to assess what the children comprehended... [SEP]\n",
      "[CLS] when did the manhattan project start and end? [SEP] best answer : the manhattan project was the project, conducted during world war ii primarily by the united states, to develop the first atomic bomb. formally designated as the manhattan engineer district ( med ), it refers specifically to the period of the project from 1942 - 1946 under the control of the u. s. army corps of engineers, under the... [SEP]\n",
      "[CLS] planet neptune : facts about its orbit, moons & rings [SEP] every 248 years, pluto moves inside neptune's orbit for 20 years or so, during which time it is closer to the sun than neptune. nevertheless, neptune remains the farthest planet from the sun, since pluto was reclassified as a dwarf planet in 2006. eptune's elliptical, oval - shaped orbit keeps the planet an average distance from the sun of almost 2. 8 billion miles ( 4. 5 billion kilometers ), or roughly 30 times as far away as earth, making it invisible to the naked eye. [SEP]\n",
      "[CLS] introduction [SEP] while communication led to the overall scientific triumph of the manhattan project, an intentional lack of communication between military and government superiors and the los alamos laboratory led to one of the most devastating moral disasters of the twentieth century. [SEP]\n",
      "[CLS] what is the manhattan project? [SEP] the manhattan project was the name for a project conducted during world war ii, to develop the first atomic bomb. it refers specifically to the period of the project from 194 … 2 - 1946 under the control of the u. s. army corps of engineers, under the administration of general leslie r. groves. [SEP]\n",
      "[CLS] the manhattan project ( film ) [SEP] the manhattan project ( film ) the manhattan project is an american film, released in 1986. named after the world war ii - era program that constructed the first atomic bombs, the plot revolves around a gifted high school student who decides to construct an atomic bomb for a national science fair. [SEP]\n",
      "[CLS] computing and the manhattan project [SEP] the manhattan project involved one of the largest scientific collaborations ever undertaken. out of it emerged countless new technologies, going far beyond the harnessing of nuclear fission. the development of early computing benefited enormously from the manhattan project ’ s innovation, especially with the los alamos laboratory ’ s developments in the field both during and after the war. [SEP]\n",
      "[CLS] what was the manhattan project? [SEP] the manhattan project was the code name for the building of the first atomic bomb. ( 1940 - 1945 ). it was put into effect by franklin delmore roosevelt, and was sought out to b … e the peace maker of world war two. [SEP]\n",
      "[CLS] atomic bombings of hiroshima and nagasaki [SEP] in preparation for dropping an atomic bomb on hiroshima, u. s. military leaders decided against a demonstration bomb, and against a special leaflet warning, in both cases because of the uncertainty of a successful detonation, and the wish to maximize psychological shock. ithin the first two to four months of the bombings, the acute effects of the atomic bombings killed 90, 000 – 146, 000 people in hiroshima and 39, 000 – 80, 000 in nagasaki ; roughly half of the deaths in each city occurred on the first day. [SEP]\n",
      "[CLS] 51g. the decision to drop the bomb [SEP] the ethical debate over the decision to drop the atomic bomb will never be resolved. the bombs did, however, bring an end to the most destructive war in history. the manhattan project that produced it demonstrated the possibility of how a nation's resources could be mobilized. pandora's box was now open. n august 9, a second atomic bomb was dropped on nagasaki, where 80, 000 japanese people perished. on august 14, 1945, the japanese surrendered. critics have charged that truman's decision was a barbaric act that brought negative long - term consequences to the united states. [SEP]\n",
      "[CLS] harry truman fast facts [SEP] one day before america entered world war ii with the bombing of pearl harbor, the manhattan project officially began with president franklin d. roosevelt's approval over the objections of some scientists including albert einstein. j. robert oppenheimer was the project's scientific director. [SEP]\n",
      "[CLS] the after - effects of the atomic bombs on hiroshima & nagasaki [SEP] on august 6, 1945, an american b - 29 bomber named the enola gay left the island of tinian for hiroshima, japan. this section recounts the first atomic bombing. hiroshima was chosen as the primary target since it had remained largely untouched by bombing raids, and the bomb ’ s effects could be clearly measured. [SEP]\n",
      "[CLS] the doolittle raid [SEP] the doolittle raid's results. the raid on tokyo was a great success. the material damage was small, but the effect on morale, both in the us and in japan, was significant and important. light over the ocean was normal. near tokyo, flying at very low altitude, the doolittle raid bombers met several formations of training aircraft, but no fighters, and no anti - aircraft fire. their surprise was perfect. it was noon, and doolittle climbed to 1200ft and dropped his first bomb over the center of tokyo. [SEP]\n",
      "[CLS] important result of the marshall plan? [SEP] effected by the united states after world war ii, the marshall plan produced the quite important result of building up nations shattered during the war. it did so quite quickly, as well. [SEP]\n",
      "[CLS] did the dropping of the atomic bomb save lives? [SEP] jul 25, 1945, the atomic bomb plan was officially approved by president truman while he was still in potsdam. 20th air force will deliver its first special bomb as soon as weather will permit visual bombing after about 3 august 1945 on one of the targets : hiroshima, kokura, niigata and nagasaki. e is honest - but smart as hell.. meanwhile, president truman was told of the successful test of the manhattan project ( atomic bomb ) in alamogordo, new mexico on jul 16, 1945. diary of president truman of jul 18, 1945 shows discussed manhattan ( it is a success ). [SEP]\n",
      "[CLS] manhattan project [SEP] the manhattan project was an epic, secret, wartime effort to design and build the world's first nuclear weapon. commanding the efforts of the world's greatest physicists and mathematicians during world war ii, the $ 20 billion project resulted in the production of the first uranium and plutonium bombs. anhattan project, the wartime effort to design and build the first nuclear weapons ( atomic bombs ). with the discovery of fission in 1939, it became clear to scientists that certain radioactive materials could be used to make a bomb of unprecented power. [SEP]\n",
      "[CLS] the story of hiroshima [SEP] the manhattan project produced two different types of atomic bombs, code - named fat man and little boy. fat man, which was dropped on nagasaki, was the more complex of the two. a bulbous, 10 - ft. bomb containing a sphere of the metal plutonium 239, it was surrounded by blocks of high explosives that were designed to produce a highly accurate and symmetrical implosion. [SEP]\n",
      "[CLS] the launch of sputnik, 1957 [SEP] the explorer was still slighter than sputnik, but its launch sent it deeper into space. the soviets responded with yet another launch, and the space race continued. the success of sputnik had a major impact on the cold war and the united states. fear that they had fallen behind led u. s. policymakers to accelerate space and weapons programs. [SEP]\n",
      "[CLS] - [SEP] the manhattan project was a secret military project created in 1942 to produce the first us nuclear weapon. fears that nazi germany would build and use a nuclear weapon during world war ii triggered the start of the manhattan project, which was originally based in manhattan, new york. [SEP]\n",
      "[CLS] 51f. the manhattan project [SEP] the main assembly plant was built at los alamos, new mexico. robert oppenheimer was put in charge of putting the pieces together at los alamos. after the final bill was tallied, nearly $ 2 billion had been spent on research and development of the atomic bomb. the manhattan project employed over 120, 000 americans. y the summer of 1945, oppenheimer was ready to test the first bomb. on july 16, 1945, at trinity site near alamogordo, new mexico, scientists of the manhattan project readied themselves to watch the detonation of the world's first atomic bomb. the device was affixed to a 100 - foot tower and [SEP]\n",
      "[CLS] the manhattan project [SEP] the program that developed the atomic bomb for the united states during world war ii was the largest secret project ever undertaken by the u. s. government. the project was originally named development of substitute materials, but there was concern that the name was too suggestive of its real purpose. section of land near alamogordo, in south - central new mexico was chosen and code - named trinity. on july 16, 1945, the world entered the nuclear age with the detonation of the first atomic bomb. the explosion created a crater which measured nearly 2, 400 feet across and was equivalent to about 20, 000 tons of tnt. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for elem in example[\"d_input_ids\"]:\n",
    "    print(tokenizer.decode(elem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": trainer._train_batch_size,\n",
    "    \"collate_fn\": data_collator,\n",
    "    \"num_workers\": trainer.args.dataloader_num_workers,\n",
    "    \"pin_memory\": trainer.args.dataloader_pin_memory,\n",
    "    \"persistent_workers\": trainer.args.dataloader_persistent_workers,\n",
    "}\n",
    "\n",
    "if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "    dataloader_params[\"sampler\"] = trainer._get_train_sampler()\n",
    "    dataloader_params[\"drop_last\"] = trainer.args.dataloader_drop_last\n",
    "    dataloader_params[\"prefetch_factor\"] = trainer.args.dataloader_prefetch_factor\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:314] 2024-11-12 13:29:48,916 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_input_ids': tensor([[  101,  2129,  2172,  2003, 24728,  8566, 11058,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2186,  2001,  7673,  9303, 10762,  1999,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2221,  2003, 10493, 12436,  2284,  1999,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  4295,  2515,  2552,  5740,  8029,  9623,  3426,  1999,\n",
       "          7125,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101, 18833,  2000, 29533,  1013, 29533,  8197,  2958,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2054,  2221,  2003, 25668,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003,  1996,  3067, 10419, 27830,  2208,  2006,  1051, 28703,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2129,  2172,  2515,  3021,  6733,  2191,  2566,  3178,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101, 12312,  5974,  4564,  2003,  1999,  2054,  3545,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003, 10514,  9688,  2102,  5960, 11948, 18780,  2496,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2129,  2172,  2515,  1996,  1048, 16846,  3465,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101, 11056,  6210,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  7473,  2290,  3574,  2005, 16928,  7242,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2190,  2833,  2076,  8249,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2003,  2009,  2995,  2008,  1996,  3009,  1011,  2137,  2162,\n",
       "          4227,  1996,  2142,  2163,  2455,  1999,  1996,  5137,  1012,   102,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2129,  2172,  2024, 26929, 12678,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]]), 'q_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'q_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'd_input_ids': tensor([[  101,  2129,  2172,  ...,     0,     0,     0],\n",
       "        [  101,  2051,  6654,  ...,     0,     0,     0],\n",
       "        [  101,  1011,   102,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1011,   102,  ...,     0,     0,     0],\n",
       "        [  101,  1011,   102,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 10026,  ...,     0,     0,     0]]), 'd_token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'd_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'kd_labels': tensor([[ 3.9209e-01, -7.1016e+00, -6.7773e+00,  2.1289e+00, -7.8086e+00,\n",
       "         -5.7031e+00, -3.1621e+00, -6.7109e+00, -5.7930e+00, -7.1992e+00,\n",
       "         -6.3633e+00, -6.3789e+00, -6.4883e+00, -6.5938e+00, -5.1719e+00,\n",
       "         -6.4336e+00, -6.6094e+00, -6.1367e+00, -6.3203e+00, -6.5312e+00,\n",
       "         -4.6953e+00, -9.2529e-01, -6.7344e+00, -6.0000e+00],\n",
       "        [ 2.5859e+00, -6.5156e+00, -7.1562e+00, -7.7266e+00, -7.4023e+00,\n",
       "         -5.9297e+00, -7.2695e+00, -7.7344e+00, -6.1602e+00, -7.7695e+00,\n",
       "         -7.5391e+00, -3.9238e+00, -7.7812e+00, -6.0625e+00, -7.6250e+00,\n",
       "         -7.0938e+00, -7.4805e+00, -6.2305e+00, -6.9062e+00, -7.8047e+00,\n",
       "         -6.9141e+00, -6.3086e+00, -7.2578e+00, -7.2500e+00],\n",
       "        [ 1.5713e+00, -7.2500e+00, -5.8828e+00, -5.9648e+00, -5.5898e+00,\n",
       "         -6.0703e+00, -5.2188e+00, -4.8047e+00, -4.8789e+00, -5.4141e+00,\n",
       "         -5.6367e+00, -4.1211e+00, -4.2734e+00, -5.7930e+00, -4.2031e+00,\n",
       "         -5.5039e+00, -6.6328e+00, -5.3008e+00, -5.2344e+00, -6.4102e+00,\n",
       "         -4.9570e+00, -7.5100e-03, -5.2500e+00, -5.4336e+00],\n",
       "        [ 1.6787e+00, -1.1787e+00, -4.9062e+00, -4.4023e+00, -5.0547e+00,\n",
       "         -4.5195e+00, -4.7109e+00, -5.1016e+00, -4.9805e+00, -3.9180e+00,\n",
       "         -4.6797e+00, -4.1328e+00, -4.6133e+00, -4.3516e+00, -4.1992e+00,\n",
       "         -4.6289e+00, -4.6133e+00, -4.0352e+00, -4.7461e+00, -4.5117e+00,\n",
       "         -2.0056e-01, -5.5508e+00, -4.9453e+00, -4.4727e+00],\n",
       "        [ 1.4424e+00, -5.5859e+00, -6.0234e+00, -5.2344e+00, -5.4180e+00,\n",
       "         -6.0977e+00, -5.7266e+00, -5.5703e+00, -5.2266e+00, -3.6035e+00,\n",
       "         -6.3320e+00, -4.4062e+00, -6.5898e+00, -6.1133e+00, -5.7383e+00,\n",
       "         -5.6797e+00, -5.5664e+00, -5.6055e+00, -5.4648e+00, -6.0547e+00,\n",
       "         -6.6992e+00, -5.8984e+00, -5.7344e+00, -5.3633e+00],\n",
       "        [ 3.5020e+00, -7.1172e+00, -5.6328e+00, -7.1914e+00, -6.4766e+00,\n",
       "         -6.8516e+00, -7.1328e+00, -7.3633e+00, -6.2344e+00, -6.5547e+00,\n",
       "         -6.3477e+00, -5.7383e+00, -5.5469e+00, -6.2305e+00, -6.1953e+00,\n",
       "         -7.0938e+00, -7.5703e+00, -7.2539e+00, -6.6211e+00, -7.4180e+00,\n",
       "         -5.7930e+00, -7.0898e+00, -6.6211e+00, -7.2070e+00],\n",
       "        [ 3.1621e+00, -7.0977e+00, -6.9688e+00, -7.5820e+00, -8.9111e-01,\n",
       "         -6.3672e+00, -6.0078e+00, -7.6328e+00, -1.4541e+00, -6.7031e+00,\n",
       "         -6.5312e+00, -6.6211e+00, -6.3594e+00, -6.5273e+00, -6.4609e+00,\n",
       "         -6.5234e+00, -5.3008e+00,  1.5693e+00, -6.4375e+00, -5.0977e+00,\n",
       "         -4.4297e+00, -6.9297e+00, -6.3867e+00, -5.9375e+00],\n",
       "        [ 4.0674e-01, -3.1992e+00, -6.2969e+00, -3.4004e+00, -4.5977e+00,\n",
       "         -5.0938e+00, -3.9473e+00, -4.1758e+00, -5.4922e+00, -4.5859e+00,\n",
       "         -5.7461e+00, -5.7422e+00, -5.2891e+00, -4.1914e+00, -4.9297e+00,\n",
       "         -4.8984e+00, -4.5625e+00, -7.1289e+00, -6.9805e+00, -4.8281e+00,\n",
       "         -3.4121e+00, -6.7500e+00, -5.5625e+00, -5.1406e+00],\n",
       "        [ 3.4531e+00, -7.5234e+00, -7.5195e+00, -7.1211e+00, -6.9297e+00,\n",
       "         -7.0938e+00, -6.7383e+00, -7.4727e+00, -7.3828e+00, -7.2422e+00,\n",
       "         -7.5742e+00, -6.9141e+00, -6.8945e+00, -7.1836e+00, -7.5430e+00,\n",
       "         -5.4180e+00, -7.1523e+00,  2.1602e+00, -6.9883e+00, -7.5742e+00,\n",
       "         -7.4609e+00, -7.4883e+00, -7.3516e+00, -7.4258e+00],\n",
       "        [ 3.5137e+00, -7.6680e+00, -7.2891e+00, -7.7891e+00, -7.9727e+00,\n",
       "         -7.5898e+00, -7.8711e+00, -7.8828e+00, -7.7461e+00, -7.7930e+00,\n",
       "         -7.6055e+00, -7.7422e+00, -7.9805e+00, -8.0156e+00, -7.9453e+00,\n",
       "         -7.8125e+00, -7.9336e+00, -7.9648e+00,  2.6250e+00, -7.9805e+00,\n",
       "         -7.6719e+00, -7.5703e+00, -7.5352e+00, -7.7188e+00],\n",
       "        [ 2.4023e+00, -7.4268e-01,  7.4902e-01, -5.1406e+00,  1.6641e+00,\n",
       "         -6.1953e+00, -2.3398e+00, -7.1016e+00, -1.1270e+00, -4.6719e+00,\n",
       "         -5.1992e+00, -5.4766e+00, -4.5977e+00, -5.0000e+00, -3.8945e+00,\n",
       "         -5.0977e+00, -3.8105e+00, -5.6211e+00, -4.2773e+00, -2.5254e+00,\n",
       "         -4.4727e+00, -2.5996e+00, -4.4961e+00, -5.2773e+00],\n",
       "        [ 2.6914e+00, -3.4492e+00, -4.1445e+00, -3.7148e+00,  9.5090e-02,\n",
       "         -4.3359e+00, -4.1055e+00, -2.7578e+00, -4.6953e+00, -4.6758e+00,\n",
       "          1.0381e+00, -3.8848e+00,  4.3945e-01, -4.6445e+00,  6.1035e-01,\n",
       "         -4.8086e+00,  2.5352e+00, -3.3711e+00, -4.0938e+00, -7.0195e+00,\n",
       "         -3.9902e+00, -5.4453e+00, -9.4580e-01, -3.7324e+00],\n",
       "        [-4.5078e+00, -5.2500e+00, -6.0938e+00, -5.4531e+00, -6.3945e+00,\n",
       "         -6.3008e+00, -4.3789e+00, -6.8008e+00, -4.6914e+00, -5.7539e+00,\n",
       "         -4.4844e+00, -6.1055e+00, -6.0352e+00, -4.5469e+00, -5.8008e+00,\n",
       "         -4.5898e+00, -5.1953e+00, -6.9922e+00, -5.3086e+00, -4.8125e+00,\n",
       "         -4.2070e+00, -5.4258e+00, -4.3750e+00, -6.6680e+00],\n",
       "        [ 2.4609e+00, -6.0938e+00, -5.0664e+00, -4.5195e+00, -5.4492e+00,\n",
       "         -2.1426e+00, -4.4844e+00, -4.8828e+00, -3.6660e+00, -4.3906e+00,\n",
       "         -4.6406e+00, -1.3877e+00, -1.6592e+00, -5.3750e+00, -4.0469e+00,\n",
       "         -6.9375e+00, -4.7695e+00, -2.8789e+00, -2.2285e+00, -4.6719e+00,\n",
       "         -4.3828e+00, -2.2188e+00, -4.5703e+00, -5.7539e+00],\n",
       "        [-1.4814e+00, -2.0801e+00, -1.7246e+00, -2.4492e+00, -2.2402e+00,\n",
       "         -3.0117e+00, -2.1367e+00, -1.0859e+00, -1.7158e+00, -2.4766e+00,\n",
       "         -2.4805e+00, -2.4258e+00, -3.0391e+00, -2.8398e+00, -3.3008e+00,\n",
       "         -2.5859e+00, -1.3018e+00, -2.6641e+00, -1.8994e+00, -1.8369e+00,\n",
       "         -1.6816e+00, -1.6328e+00, -1.6670e+00, -2.7559e+00],\n",
       "        [-6.9580e-01, -7.3682e-01, -1.8164e+00, -2.1660e+00, -7.3203e+00,\n",
       "         -2.5723e+00, -6.2939e-01, -2.3867e+00, -9.1455e-01, -5.2930e-01,\n",
       "         -1.3682e+00, -1.0449e+00, -1.6523e+00, -7.0898e-01, -1.3096e+00,\n",
       "         -1.3877e+00, -1.7031e+00, -2.4922e+00, -1.8301e+00, -2.0469e+00,\n",
       "         -1.9883e+00, -1.4062e+00, -7.2461e+00, -2.6790e-02]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"q_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 144])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"d_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2172,  2003, 24728,  8566, 11058,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2054,  2186,  2001,  7673,  9303, 10762,  1999,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2054,  2221,  2003, 10493, 12436,  2284,  1999,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2054,  4295,  2515,  2552,  5740,  8029,  9623,  3426,  1999,\n",
       "           7125,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 18833,  2000, 29533,  1013, 29533,  8197,  2958,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2054,  2221,  2003, 25668,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2003,  1996,  3067, 10419, 27830,  2208,  2006,  1051, 28703,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2129,  2172,  2515,  3021,  6733,  2191,  2566,  3178,  1029,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 12312,  5974,  4564,  2003,  1999,  2054,  3545,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2003, 10514,  9688,  2102,  5960, 11948, 18780,  2496,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2129,  2172,  2515,  1996,  1048, 16846,  3465,  1029,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101, 11056,  6210,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  7473,  2290,  3574,  2005, 16928,  7242,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2190,  2833,  2076,  8249,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2003,  2009,  2995,  2008,  1996,  3009,  1011,  2137,  2162,\n",
       "           4227,  1996,  2142,  2163,  2455,  1999,  1996,  5137,  1012,   102,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2129,  2172,  2024, 26929, 12678,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'kd_labels': tensor([[ 3.9209e-01, -7.1016e+00, -6.7773e+00,  2.1289e+00, -7.8086e+00,\n",
       "          -5.7031e+00, -3.1621e+00, -6.7109e+00, -5.7930e+00, -7.1992e+00,\n",
       "          -6.3633e+00, -6.3789e+00, -6.4883e+00, -6.5938e+00, -5.1719e+00,\n",
       "          -6.4336e+00, -6.6094e+00, -6.1367e+00, -6.3203e+00, -6.5312e+00,\n",
       "          -4.6953e+00, -9.2529e-01, -6.7344e+00, -6.0000e+00],\n",
       "         [ 2.5859e+00, -6.5156e+00, -7.1562e+00, -7.7266e+00, -7.4023e+00,\n",
       "          -5.9297e+00, -7.2695e+00, -7.7344e+00, -6.1602e+00, -7.7695e+00,\n",
       "          -7.5391e+00, -3.9238e+00, -7.7812e+00, -6.0625e+00, -7.6250e+00,\n",
       "          -7.0938e+00, -7.4805e+00, -6.2305e+00, -6.9062e+00, -7.8047e+00,\n",
       "          -6.9141e+00, -6.3086e+00, -7.2578e+00, -7.2500e+00],\n",
       "         [ 1.5713e+00, -7.2500e+00, -5.8828e+00, -5.9648e+00, -5.5898e+00,\n",
       "          -6.0703e+00, -5.2188e+00, -4.8047e+00, -4.8789e+00, -5.4141e+00,\n",
       "          -5.6367e+00, -4.1211e+00, -4.2734e+00, -5.7930e+00, -4.2031e+00,\n",
       "          -5.5039e+00, -6.6328e+00, -5.3008e+00, -5.2344e+00, -6.4102e+00,\n",
       "          -4.9570e+00, -7.5100e-03, -5.2500e+00, -5.4336e+00],\n",
       "         [ 1.6787e+00, -1.1787e+00, -4.9062e+00, -4.4023e+00, -5.0547e+00,\n",
       "          -4.5195e+00, -4.7109e+00, -5.1016e+00, -4.9805e+00, -3.9180e+00,\n",
       "          -4.6797e+00, -4.1328e+00, -4.6133e+00, -4.3516e+00, -4.1992e+00,\n",
       "          -4.6289e+00, -4.6133e+00, -4.0352e+00, -4.7461e+00, -4.5117e+00,\n",
       "          -2.0056e-01, -5.5508e+00, -4.9453e+00, -4.4727e+00],\n",
       "         [ 1.4424e+00, -5.5859e+00, -6.0234e+00, -5.2344e+00, -5.4180e+00,\n",
       "          -6.0977e+00, -5.7266e+00, -5.5703e+00, -5.2266e+00, -3.6035e+00,\n",
       "          -6.3320e+00, -4.4062e+00, -6.5898e+00, -6.1133e+00, -5.7383e+00,\n",
       "          -5.6797e+00, -5.5664e+00, -5.6055e+00, -5.4648e+00, -6.0547e+00,\n",
       "          -6.6992e+00, -5.8984e+00, -5.7344e+00, -5.3633e+00],\n",
       "         [ 3.5020e+00, -7.1172e+00, -5.6328e+00, -7.1914e+00, -6.4766e+00,\n",
       "          -6.8516e+00, -7.1328e+00, -7.3633e+00, -6.2344e+00, -6.5547e+00,\n",
       "          -6.3477e+00, -5.7383e+00, -5.5469e+00, -6.2305e+00, -6.1953e+00,\n",
       "          -7.0938e+00, -7.5703e+00, -7.2539e+00, -6.6211e+00, -7.4180e+00,\n",
       "          -5.7930e+00, -7.0898e+00, -6.6211e+00, -7.2070e+00],\n",
       "         [ 3.1621e+00, -7.0977e+00, -6.9688e+00, -7.5820e+00, -8.9111e-01,\n",
       "          -6.3672e+00, -6.0078e+00, -7.6328e+00, -1.4541e+00, -6.7031e+00,\n",
       "          -6.5312e+00, -6.6211e+00, -6.3594e+00, -6.5273e+00, -6.4609e+00,\n",
       "          -6.5234e+00, -5.3008e+00,  1.5693e+00, -6.4375e+00, -5.0977e+00,\n",
       "          -4.4297e+00, -6.9297e+00, -6.3867e+00, -5.9375e+00],\n",
       "         [ 4.0674e-01, -3.1992e+00, -6.2969e+00, -3.4004e+00, -4.5977e+00,\n",
       "          -5.0938e+00, -3.9473e+00, -4.1758e+00, -5.4922e+00, -4.5859e+00,\n",
       "          -5.7461e+00, -5.7422e+00, -5.2891e+00, -4.1914e+00, -4.9297e+00,\n",
       "          -4.8984e+00, -4.5625e+00, -7.1289e+00, -6.9805e+00, -4.8281e+00,\n",
       "          -3.4121e+00, -6.7500e+00, -5.5625e+00, -5.1406e+00],\n",
       "         [ 3.4531e+00, -7.5234e+00, -7.5195e+00, -7.1211e+00, -6.9297e+00,\n",
       "          -7.0938e+00, -6.7383e+00, -7.4727e+00, -7.3828e+00, -7.2422e+00,\n",
       "          -7.5742e+00, -6.9141e+00, -6.8945e+00, -7.1836e+00, -7.5430e+00,\n",
       "          -5.4180e+00, -7.1523e+00,  2.1602e+00, -6.9883e+00, -7.5742e+00,\n",
       "          -7.4609e+00, -7.4883e+00, -7.3516e+00, -7.4258e+00],\n",
       "         [ 3.5137e+00, -7.6680e+00, -7.2891e+00, -7.7891e+00, -7.9727e+00,\n",
       "          -7.5898e+00, -7.8711e+00, -7.8828e+00, -7.7461e+00, -7.7930e+00,\n",
       "          -7.6055e+00, -7.7422e+00, -7.9805e+00, -8.0156e+00, -7.9453e+00,\n",
       "          -7.8125e+00, -7.9336e+00, -7.9648e+00,  2.6250e+00, -7.9805e+00,\n",
       "          -7.6719e+00, -7.5703e+00, -7.5352e+00, -7.7188e+00],\n",
       "         [ 2.4023e+00, -7.4268e-01,  7.4902e-01, -5.1406e+00,  1.6641e+00,\n",
       "          -6.1953e+00, -2.3398e+00, -7.1016e+00, -1.1270e+00, -4.6719e+00,\n",
       "          -5.1992e+00, -5.4766e+00, -4.5977e+00, -5.0000e+00, -3.8945e+00,\n",
       "          -5.0977e+00, -3.8105e+00, -5.6211e+00, -4.2773e+00, -2.5254e+00,\n",
       "          -4.4727e+00, -2.5996e+00, -4.4961e+00, -5.2773e+00],\n",
       "         [ 2.6914e+00, -3.4492e+00, -4.1445e+00, -3.7148e+00,  9.5090e-02,\n",
       "          -4.3359e+00, -4.1055e+00, -2.7578e+00, -4.6953e+00, -4.6758e+00,\n",
       "           1.0381e+00, -3.8848e+00,  4.3945e-01, -4.6445e+00,  6.1035e-01,\n",
       "          -4.8086e+00,  2.5352e+00, -3.3711e+00, -4.0938e+00, -7.0195e+00,\n",
       "          -3.9902e+00, -5.4453e+00, -9.4580e-01, -3.7324e+00],\n",
       "         [-4.5078e+00, -5.2500e+00, -6.0938e+00, -5.4531e+00, -6.3945e+00,\n",
       "          -6.3008e+00, -4.3789e+00, -6.8008e+00, -4.6914e+00, -5.7539e+00,\n",
       "          -4.4844e+00, -6.1055e+00, -6.0352e+00, -4.5469e+00, -5.8008e+00,\n",
       "          -4.5898e+00, -5.1953e+00, -6.9922e+00, -5.3086e+00, -4.8125e+00,\n",
       "          -4.2070e+00, -5.4258e+00, -4.3750e+00, -6.6680e+00],\n",
       "         [ 2.4609e+00, -6.0938e+00, -5.0664e+00, -4.5195e+00, -5.4492e+00,\n",
       "          -2.1426e+00, -4.4844e+00, -4.8828e+00, -3.6660e+00, -4.3906e+00,\n",
       "          -4.6406e+00, -1.3877e+00, -1.6592e+00, -5.3750e+00, -4.0469e+00,\n",
       "          -6.9375e+00, -4.7695e+00, -2.8789e+00, -2.2285e+00, -4.6719e+00,\n",
       "          -4.3828e+00, -2.2188e+00, -4.5703e+00, -5.7539e+00],\n",
       "         [-1.4814e+00, -2.0801e+00, -1.7246e+00, -2.4492e+00, -2.2402e+00,\n",
       "          -3.0117e+00, -2.1367e+00, -1.0859e+00, -1.7158e+00, -2.4766e+00,\n",
       "          -2.4805e+00, -2.4258e+00, -3.0391e+00, -2.8398e+00, -3.3008e+00,\n",
       "          -2.5859e+00, -1.3018e+00, -2.6641e+00, -1.8994e+00, -1.8369e+00,\n",
       "          -1.6816e+00, -1.6328e+00, -1.6670e+00, -2.7559e+00],\n",
       "         [-6.9580e-01, -7.3682e-01, -1.8164e+00, -2.1660e+00, -7.3203e+00,\n",
       "          -2.5723e+00, -6.2939e-01, -2.3867e+00, -9.1455e-01, -5.2930e-01,\n",
       "          -1.3682e+00, -1.0449e+00, -1.6523e+00, -7.0898e-01, -1.3096e+00,\n",
       "          -1.3877e+00, -1.7031e+00, -2.4922e+00, -1.8301e+00, -2.0469e+00,\n",
       "          -1.9883e+00, -1.4062e+00, -7.2461e+00, -2.6790e-02]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trainers.biencoder_trainer import _unpack_qp\n",
    "query_batch_dict, doc_batch_dict = _unpack_qp(batch)\n",
    "query_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "def init_distributed_single_gpu():\n",
    "    # Set environment variables\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    \n",
    "    # Initialize process group\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",  # Use NCCL backend for GPU\n",
    "        rank=0,          # Single GPU, so rank is 0\n",
    "        world_size=1     # Total number of processes is 1\n",
    "    )\n",
    "    \n",
    "    # Set the device\n",
    "    torch.cuda.set_device(1)\n",
    "\n",
    "init_distributed_single_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = model.to(DEVICE)\n",
    "query_batch_dict = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in query_batch_dict.items()}\n",
    "doc_batch_dict = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in doc_batch_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiencoderOutput(q_reps=tensor([[ 0.0417, -0.0576,  0.0260,  ..., -0.0078,  0.0197,  0.0434],\n",
       "        [-0.0273, -0.0229, -0.0409,  ...,  0.0051,  0.0109,  0.0149],\n",
       "        [ 0.0280,  0.0017,  0.0068,  ..., -0.0052,  0.0060, -0.0040],\n",
       "        ...,\n",
       "        [ 0.0115,  0.0308, -0.0189,  ..., -0.0083,  0.0140, -0.0190],\n",
       "        [-0.0075, -0.0525, -0.0408,  ..., -0.0002,  0.0039, -0.0084],\n",
       "        [ 0.0630,  0.0090, -0.0447,  ..., -0.0181,  0.0172, -0.0029]],\n",
       "       device='cuda:1', grad_fn=<DivBackward0>), p_reps=tensor([[-0.0107, -0.0948,  0.0390,  ...,  0.0099,  0.0322,  0.0229],\n",
       "        [-0.0070, -0.0582,  0.0135,  ...,  0.0241, -0.0216,  0.0216],\n",
       "        [-0.0309, -0.0687,  0.0285,  ..., -0.0108, -0.0566,  0.0010],\n",
       "        ...,\n",
       "        [ 0.0161, -0.0075, -0.0256,  ..., -0.0083,  0.0362, -0.0082],\n",
       "        [-0.0584, -0.0090, -0.0012,  ...,  0.0298,  0.0180,  0.0149],\n",
       "        [ 0.0006, -0.0477, -0.0298,  ...,  0.0213,  0.0414,  0.0023]],\n",
       "       device='cuda:1', grad_fn=<DivBackward0>), loss=tensor(7.3531, device='cuda:1', grad_fn=<AddBackward0>), labels=tensor([  0,  24,  48,  72,  96, 120, 144, 168, 192, 216, 240, 264, 288, 312,\n",
       "        336, 360], device='cuda:1'), scores=tensor([[25.3413, 15.2483, 20.4754,  ..., 16.7519, 12.3674, 13.6156],\n",
       "        [15.7347, 12.6970, 15.8135,  ..., 15.7178, 14.2417, 13.2474],\n",
       "        [15.2907, 11.3774, 13.4731,  ..., 14.1376, 13.5927, 13.8204],\n",
       "        ...,\n",
       "        [15.2713, 13.3434, 16.9948,  ..., 16.9970, 14.4358, 15.3456],\n",
       "        [15.0980, 14.0180, 16.6975,  ..., 17.3544, 17.9439, 12.3854],\n",
       "        [15.1910, 12.0458, 15.7462,  ..., 20.8093, 11.8675, 20.2737]],\n",
       "       device='cuda:1', grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(query=query_batch_dict, passage=doc_batch_dict)\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
