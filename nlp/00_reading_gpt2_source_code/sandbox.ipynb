{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/miniconda3/envs/gpt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "text = \"In a shocking finding, scientists\"\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), tensor([[  818,   257, 14702,  4917,    11,  5519]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out how tokenized input looks like\n",
    "inputs.shape, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n"
     ]
    }
   ],
   "source": [
    "# turning tokens back into text\n",
    "print(tokenizer.decode([818]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      " a\n",
      " shocking\n",
      " finding\n",
      ",\n",
      " scientists\n"
     ]
    }
   ],
   "source": [
    "# seeing how tokenization is done\n",
    "for token in inputs[0]:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1169, 262]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# begin of sentence token id is slightly different!\n",
    "tokenizer.encode(\"the the the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', ' the')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1169]), tokenizer.decode([262])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161, 245, 250]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some characters may spand multiple tokens\n",
    "tokenizer.encode(\"嗜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 269, 327, 327]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization is case sensitive\n",
    "tokenizer.encode(\"c c C C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('�', '�', '�')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([161]), tokenizer.decode([245]), tokenizer.decode([250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientists have found that the brain of a baby born with Down syndrome is not fully developed until the age of two.\n",
      "\n",
      "The study, published in the journal Nature, found that the brain of a baby born with Down syndrome is not fully developed until the age of two.\n",
      "\n",
      "The findings, which were made by scientists at the University of California, San Francisco, and the University of California, San Diego, are the first to show that the brain of a baby born with Down syndrome is not fully developed until the age of two.\n",
      "\n",
      "The study, which was carried out on mice, found that the brain of a baby born with Down syndrome is not fully developed until the age of two. The findings, which were made by scientists at the University of California, San Francisco, and the University of California, San Diego, are the first to show that the brain of a baby born with Down syndrome is not fully developed until the age of two. The study, which was carried out on mice, found that the brain of a baby born with Down syndrome is not fully developed until the age of two. The findings, which were made by scientists at the University of California, San Francisco, and the University of California"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=250, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]),\n",
       " torch.Size([1, 6, 50257]),\n",
       " tensor([[[-0.2192,  1.7349, -2.3250,  ..., -5.6592, -7.6884, -0.9767],\n",
       "          [-1.1942, -0.1058, -4.2319,  ..., -6.0229, -5.4356, -0.9255],\n",
       "          [ 0.5741,  1.6945, -3.0760,  ..., -4.9000, -7.8580, -1.5279],\n",
       "          [ 2.0346,  2.1569, -3.5765,  ..., -5.8734, -6.3885, -0.0463],\n",
       "          [-1.7774, -1.2454, -4.8257,  ..., -2.1206, -6.7244, -1.6851],\n",
       "          [ 0.7919,  0.9766, -5.5937,  ..., -5.7744, -1.8508,  0.0856]]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling actual model\n",
    "preds = model(inputs).logits\n",
    "inputs.shape, preds.shape, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' have'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out generated token\n",
    "tokenizer.decode(preds[0][-1].argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
